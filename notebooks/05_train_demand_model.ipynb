{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7a7e549",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nDemand Forecasting Model Training Script\\nThis script trains an XGBoost model for energy demand forecasting using the processed and analyzed demand data.\\n'"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Demand Forecasting Model Training Script\n",
    "This script trains an XGBoost model for energy demand forecasting using the processed and analyzed demand data.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7c580bbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e7f5e810",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3a5f2515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths - relative to script location\n",
    "BASE_DIR = Path(os.getcwd()).parent\n",
    "DATA_DIR = BASE_DIR / 'data/final/Italy'\n",
    "MODELS_DIR = BASE_DIR / 'models'\n",
    "IMAGES_DIR = BASE_DIR / 'outputs/images'\n",
    "OUTPUT_DIR = BASE_DIR / 'outputs'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8310bff0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "# Create output directories\n",
    "for directory in [MODELS_DIR, IMAGES_DIR, OUTPUT_DIR]:\n",
    "    os.makedirs(directory, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "601c267d",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load the processed demand data with engineered features\"\"\"\n",
    "    print(\"Loading processed demand data...\")\n",
    "    \n",
    "\n",
    "    train_data = pd.read_csv(DATA_DIR / 'demand_train_data.csv')\n",
    "    test_data = pd.read_csv(DATA_DIR / 'demand_test_data.csv')\n",
    "    \n",
    "    # Convert date column to datetime\n",
    "    train_data['date'] = pd.to_datetime(train_data['date'])\n",
    "    test_data['date'] = pd.to_datetime(test_data['date'])\n",
    "    \n",
    "    print(f\"Successfully loaded training data: {train_data.shape[0]} rows\")\n",
    "    print(f\"Successfully loaded testing data: {test_data.shape[0]} rows\")\n",
    "    \n",
    "    return train_data, test_data\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ad76ead7",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "daf467b0",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def prepare_features_target(train_data, test_data):\n",
    "    \"\"\"Prepare features and target variables for modeling\"\"\"\n",
    "    print(\"Preparing features and target variables...\")\n",
    "    \n",
    "    # Define features to use\n",
    "    features = [\n",
    "        'dayofweek', 'month', 'quarter', 'year', 'dayofyear', 'is_weekend',\n",
    "        'demand_lag1', 'demand_lag7', 'demand_lag30',\n",
    "        'demand_rolling_7d_mean', 'demand_rolling_30d_mean',\n",
    "        'demand_rolling_7d_std', 'demand_rolling_30d_std',\n",
    "        'price_eur_mwh'\n",
    "    ]\n",
    "    \n",
    "    # Ensure all features exist in the dataframes\n",
    "    features_to_remove = []\n",
    "    for feature in features:\n",
    "        if feature not in train_data.columns:\n",
    "            print(f\"Warning: Feature '{feature}' not found in training data. Will be removed from feature list.\")\n",
    "            features_to_remove.append(feature)\n",
    "        elif feature not in test_data.columns:\n",
    "            print(f\"Warning: Feature '{feature}' not found in test data. Will be removed from feature list.\")\n",
    "            if feature not in features_to_remove:\n",
    "                features_to_remove.append(feature)\n",
    "\n",
    "    for feature in features_to_remove:\n",
    "        features.remove(feature)\n",
    "\n",
    "    if not features:\n",
    "        raise ValueError(\"No features selected or available in the data. Halting.\")\n",
    "\n",
    "    print(f\"Attempting to use features: {features}\")\n",
    "    \n",
    "    # Save feature list for future use\n",
    "    joblib.dump(features, MODELS_DIR / 'demand_features.joblib')\n",
    "    \n",
    "    # Prepare training data\n",
    "    X_train = train_data[features]\n",
    "    y_train = train_data['Demand']\n",
    "    \n",
    "    # Prepare testing data\n",
    "    X_test = test_data[features]\n",
    "    y_test = test_data['Demand']\n",
    "    \n",
    "    print(f\"Features used: {features}\")\n",
    "    print(f\"X_train shape: {X_train.shape}, y_train shape: {y_train.shape}\")\n",
    "    print(f\"X_test shape: {X_test.shape}, y_test shape: {y_test.shape}\")\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, features\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "f02b8fa4",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "def train_model(X_train, y_train, X_test, y_test):\n",
    "    \"\"\"Train XGBoost model for demand forecasting\"\"\"\n",
    "    print(\"Training XGBoost model...\")\n",
    "    \n",
    "    # Define XGBoost model parameters\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=1000,\n",
    "        learning_rate=0.05,\n",
    "        max_depth=5,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        objective='reg:squarederror',\n",
    "        eval_metric='rmse',\n",
    "        early_stopping_rounds=50,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    # Train the model with early stopping\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_test, y_test)],\n",
    "        verbose=True\n",
    "    )\n",
    "    \n",
    "    # Evaluate on test set\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    print(f\"Test R² score: {test_score:.4f}\")\n",
    "    \n",
    "    # Save the model\n",
    "    model_path = MODELS_DIR / 'energy_demand_xgb_v1.joblib'\n",
    "    joblib.dump(model, model_path)\n",
    "    print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "def evaluate_model(model, X_test, y_test, features):\n",
    "    \"\"\"Evaluate model performance on test data\"\"\"\n",
    "    print(\"Evaluating model performance...\")\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_test)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    \n",
    "    print(f\"RMSE: {rmse:.2f} MWh\")\n",
    "    print(f\"MAE: {mae:.2f} MWh\")\n",
    "    print(f\"R²: {r2:.4f}\")\n",
    "    \n",
    "    # Create a dataframe with actual and predicted values\n",
    "    results_df = pd.DataFrame({\n",
    "        'Actual': y_test,\n",
    "        'Predicted': y_pred\n",
    "    })\n",
    "    \n",
    "    # Plot actual vs predicted\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(results_df.index, results_df['Actual'], label='Actual', color='blue')\n",
    "    plt.plot(results_df.index, results_df['Predicted'], label='Predicted', color='red')\n",
    "    plt.title('Actual vs Predicted Energy Demand')\n",
    "    plt.xlabel('Sample Index')\n",
    "    plt.ylabel('Demand (MWh)')\n",
    "    plt.legend()\n",
    "    plt.savefig(IMAGES_DIR / 'demand_actual_vs_predicted.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot scatter of actual vs predicted\n",
    "    plt.figure(figsize=(10, 10))\n",
    "    plt.scatter(results_df['Actual'], results_df['Predicted'], alpha=0.5)\n",
    "    plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "    plt.title('Actual vs Predicted Energy Demand')\n",
    "    plt.xlabel('Actual Demand (MWh)')\n",
    "    plt.ylabel('Predicted Demand (MWh)')\n",
    "    plt.savefig(IMAGES_DIR / 'demand_scatter_actual_vs_predicted.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    xgb.plot_importance(model, max_num_features=len(features))\n",
    "    plt.title('Feature Importance')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(IMAGES_DIR / 'demand_feature_importance.png', dpi=300)\n",
    "    plt.close()\n",
    "    \n",
    "    return rmse, mae, r2\n",
    "\n",
    "def load_price_forecasts():\n",
    "    \"\"\"Load the price forecasts for 2025-2029\"\"\"\n",
    "    print(\"Loading price forecasts for 2025-2029...\")\n",
    "    \n",
    "    try:\n",
    "        price_forecasts = pd.read_csv(DATA_DIR / 'energy_price2025_2029.csv')\n",
    "        price_forecasts['Date'] = pd.to_datetime(price_forecasts['Date'])\n",
    "        print(f\"Successfully loaded price forecasts: {price_forecasts.shape[0]} rows\")\n",
    "        return price_forecasts\n",
    "    except FileNotFoundError:\n",
    "        print(\"Price forecast file not found. Please ensure the file exists.\")\n",
    "        return None\n",
    "\n",
    "def generate_future_predictions(model, features, price_forecasts):\n",
    "    \"\"\"\n",
    "    Generate demand predictions for future dates using the trained model.\n",
    "    \n",
    "    Args:\n",
    "        model: Trained XGBoost model\n",
    "        features: List of features used by the model\n",
    "        price_forecasts: DataFrame containing price forecasts for future dates\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with future demand predictions\n",
    "    \"\"\"\n",
    "    print(\"Generating future demand predictions for 2025-2029...\")\n",
    "    \n",
    "    if price_forecasts is None:\n",
    "        print(\"Error: Price forecasts not available. Cannot generate future predictions.\")\n",
    "        return None\n",
    "    \n",
    "    # Create a copy of the price forecasts dataframe\n",
    "    future_df = price_forecasts.copy()\n",
    "    \n",
    "    # Ensure the date column is in datetime format\n",
    "    future_df['Date'] = pd.to_datetime(future_df['Date'])\n",
    "    future_df.set_index('Date', inplace=True)\n",
    "    \n",
    "    # Create time-based features\n",
    "    future_df['dayofweek'] = future_df.index.dayofweek\n",
    "    future_df['month'] = future_df.index.month\n",
    "    future_df['quarter'] = future_df.index.quarter\n",
    "    future_df['year'] = future_df.index.year\n",
    "    future_df['dayofyear'] = future_df.index.dayofyear\n",
    "    \n",
    "    # Initialize lag features with NaN\n",
    "    future_df['demand_lag1'] = np.nan\n",
    "    future_df['demand_lag7'] = np.nan\n",
    "    future_df['demand_lag30'] = np.nan\n",
    "    future_df['demand_rolling_7d_mean'] = np.nan\n",
    "    future_df['demand_rolling_30d_mean'] = np.nan\n",
    "    future_df['demand_rolling_7d_std'] = np.nan\n",
    "    future_df['demand_rolling_30d_std'] = np.nan\n",
    "    \n",
    "    # Load historical data to initialize lag features\n",
    "  \n",
    "    historical_data = pd.read_csv(os.path.join(DATA_DIR, \"demand_train_data.csv\"))\n",
    "    \n",
    "    # Handle different column name cases\n",
    "    date_col = None\n",
    "    demand_col = None\n",
    "    \n",
    "    # Find date column\n",
    "    for col in historical_data.columns:\n",
    "        if col.lower() in ['date', 'datetime', 'time']:\n",
    "            date_col = col\n",
    "            break\n",
    "    \n",
    "    # Find demand column\n",
    "    for col in historical_data.columns:\n",
    "        if col.lower() in ['demand', 'energy_demand', 'energy']:\n",
    "            demand_col = col\n",
    "            break\n",
    "            \n",
    "    if date_col is None or demand_col is None:\n",
    "        raise ValueError(f\"Could not identify date or demand columns. Available columns: {historical_data.columns}\")\n",
    "        \n",
    "    historical_data[date_col] = pd.to_datetime(historical_data[date_col])\n",
    "    historical_data.set_index(date_col, inplace=True)\n",
    "    historical_data = historical_data.sort_index()\n",
    "    \n",
    "    # Get the last 30 days of historical demand\n",
    "    last_30_days = historical_data.tail(30)\n",
    "    \n",
    "    # Initialize lag values from historical data\n",
    "    last_demands = last_30_days[demand_col].values\n",
    "    \n",
    "    # Set initial lag values for the first day in the future dataset\n",
    "    future_df.iloc[0, future_df.columns.get_loc('demand_lag1')] = last_demands[-1]\n",
    "    future_df.iloc[0, future_df.columns.get_loc('demand_lag7')] = last_demands[-7] if len(last_demands) >= 7 else last_demands[-1]\n",
    "    future_df.iloc[0, future_df.columns.get_loc('demand_lag30')] = last_demands[-30] if len(last_demands) >= 30 else last_demands[-1]\n",
    "    \n",
    "    # Set initial rolling statistics\n",
    "    future_df.iloc[0, future_df.columns.get_loc('demand_rolling_7d_mean')] = np.mean(last_demands[-7:])\n",
    "    future_df.iloc[0, future_df.columns.get_loc('demand_rolling_30d_mean')] = np.mean(last_demands)\n",
    "    future_df.iloc[0, future_df.columns.get_loc('demand_rolling_7d_std')] = np.std(last_demands[-7:])\n",
    "    future_df.iloc[0, future_df.columns.get_loc('demand_rolling_30d_std')] = np.std(last_demands)\n",
    "        \n",
    "\n",
    "    \n",
    "    # Add a Demand column to store predictions\n",
    "    future_df['Demand'] = np.nan\n",
    "    \n",
    "    # Iteratively predict each day\n",
    "    for i in range(len(future_df)):\n",
    "        # Get features for current prediction\n",
    "        X_pred = future_df.iloc[i:i+1][features].copy()\n",
    "        \n",
    "\n",
    "        # Make prediction\n",
    "        pred = model.predict(X_pred)[0]\n",
    "        \n",
    "        # Store prediction\n",
    "        future_df.iloc[i, future_df.columns.get_loc('Demand')] = pred\n",
    "        \n",
    "        # Update lag features for next day if not the last day\n",
    "        if i < len(future_df) - 1:\n",
    "            future_df.iloc[i+1, future_df.columns.get_loc('demand_lag1')] = pred\n",
    "            \n",
    "            # Update lag7 - either from prediction or from historical data\n",
    "            if i >= 6:\n",
    "                future_df.iloc[i+1, future_df.columns.get_loc('demand_lag7')] = future_df.iloc[i-6, future_df.columns.get_loc('Demand')]\n",
    "            \n",
    "            # Update lag30 - either from prediction or from historical data\n",
    "            if i >= 29:\n",
    "                future_df.iloc[i+1, future_df.columns.get_loc('demand_lag30')] = future_df.iloc[i-29, future_df.columns.get_loc('Demand')]\n",
    "            \n",
    "            # Update rolling statistics\n",
    "            if i >= 6:\n",
    "                last_7_days = future_df.iloc[i-6:i+1]['Demand'].values\n",
    "                future_df.iloc[i+1, future_df.columns.get_loc('demand_rolling_7d_mean')] = np.mean(last_7_days)\n",
    "                future_df.iloc[i+1, future_df.columns.get_loc('demand_rolling_7d_std')] = np.std(last_7_days)\n",
    "            \n",
    "            if i >= 29:\n",
    "                last_30_days = future_df.iloc[i-29:i+1]['Demand'].values\n",
    "                future_df.iloc[i+1, future_df.columns.get_loc('demand_rolling_30d_mean')] = np.mean(last_30_days)\n",
    "                future_df.iloc[i+1, future_df.columns.get_loc('demand_rolling_30d_std')] = np.std(last_30_days)\n",
    "        \n",
    "        # Print progress\n",
    "        if (i+1) % 100 == 0 or i == len(future_df) - 1:\n",
    "            print(f\"Progress: {i+1}/{len(future_df)} days processed\")\n",
    "    \n",
    "    # Reset index to have Date as a column\n",
    "    future_df = future_df.reset_index()\n",
    "    \n",
    "    # Save predictions - check if Price column exists or has a different name\n",
    "    output_path = os.path.join(DATA_DIR, \"energy_demand2025_2029.csv\")\n",
    "    \n",
    "    # Check if 'Price' column exists\n",
    "    if 'Price' in future_df.columns:\n",
    "        future_df[['Date', 'Demand', 'Price']].to_csv(output_path, index=False)\n",
    "    else:\n",
    "        # Look for alternative price column names\n",
    "        price_cols = [col for col in future_df.columns if 'price' in col.lower()]\n",
    "        if price_cols:\n",
    "            # Use the first found price column\n",
    "            future_df[['Date', 'Demand', price_cols[0]]].to_csv(output_path, index=False)\n",
    "            print(f\"Used '{price_cols[0]}' as the price column\")\n",
    "        else:\n",
    "            # Save without price column\n",
    "            future_df[['Date', 'Demand']].to_csv(output_path, index=False)\n",
    "            print(\"Warning: No price column found in the data\")\n",
    "    \n",
    "    print(f\"Future demand predictions saved to {output_path}\")\n",
    "    \n",
    "    return future_df\n",
    "\n",
    "def analyze_correlations(historical_data, future_predictions):\n",
    "    \"\"\"\n",
    "    Analyze correlations between energy price and demand in historical and future data.\n",
    "    \n",
    "    Args:\n",
    "        historical_data: DataFrame with historical demand data (should include price)\n",
    "        future_predictions: DataFrame with future demand predictions\n",
    "    \"\"\"\n",
    "    print(\"Analyzing correlations between price and demand...\")\n",
    "    \n",
    "    # Historical correlation\n",
    "    try:\n",
    "        # historical_data (train_data) should already be the result of a merge\n",
    "        # and contain both demand and price information.\n",
    "\n",
    "        # Find relevant columns in historical_data\n",
    "        demand_col_hist = None\n",
    "        price_col_hist = None\n",
    "\n",
    "        for col in historical_data.columns:\n",
    "            if col.lower() in ['demand', 'energy_demand', 'energy']: # Should find 'Demand'\n",
    "                demand_col_hist = col\n",
    "                break\n",
    "        \n",
    "        for col in historical_data.columns:\n",
    "            if 'price_eur_mwh' == col.lower(): # Look for the specific price column\n",
    "                price_col_hist = col\n",
    "                break\n",
    "            elif 'price' in col.lower() and price_col_hist is None: # Fallback if exact name isn't found\n",
    "                price_col_hist = col\n",
    "\n",
    "\n",
    "        if demand_col_hist is None or price_col_hist is None:\n",
    "            print(f\"Could not identify demand or price columns in historical_data. Available columns: {historical_data.columns}\")\n",
    "            # If price is missing, we can't do this part of the analysis.\n",
    "            # Consider what historical_data is being passed if this happens.\n",
    "            # It should be train_data, which comes from demand_train_data.csv,\n",
    "            # which should come from the _merged.csv file.\n",
    "            if price_col_hist is None:\n",
    "                print(\"Price column not found in historical_data for correlation analysis.\")\n",
    "                raise KeyError(\"Price column for historical correlation not found in input historical_data.\")\n",
    "\n",
    "\n",
    "        # Calculate correlation directly from historical_data\n",
    "        correlation = historical_data[demand_col_hist].corr(historical_data[price_col_hist])\n",
    "        print(f\"Historical correlation between price and demand: {correlation:.4f}\")\n",
    "        \n",
    "        # Create scatter plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(historical_data[price_col_hist], historical_data[demand_col_hist], alpha=0.5)\n",
    "        plt.title('Historical Price vs Demand Correlation')\n",
    "        plt.xlabel('Price (EUR/MWh)')\n",
    "        plt.ylabel('Demand (MWh)')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(IMAGES_DIR, 'historical_price_demand_correlation.png'), dpi=300)\n",
    "        plt.close() # Changed from plt.show() to plt.close() to match other plots\n",
    "\n",
    "        \n",
    "    except KeyError as ke:\n",
    "        print(f\"KeyError during historical correlation analysis: {ke}. Check column names in historical_data.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not analyze historical correlation: {e}\")\n",
    "    \n",
    "    # Future correlation\n",
    "    try:\n",
    "        # Find price column in future predictions\n",
    "        price_col_future = None\n",
    "        \n",
    "        for col in future_predictions.columns:\n",
    "            if 'price' in col.lower():\n",
    "                price_col_future = col\n",
    "                break\n",
    "                \n",
    "        if price_col_future is None:\n",
    "            print(\"Could not identify price column in future predictions\")\n",
    "            return\n",
    "            \n",
    "        # Calculate correlation\n",
    "        correlation = future_predictions['Demand'].corr(future_predictions[price_col_future])\n",
    "        print(f\"Future correlation between price and demand: {correlation:.4f}\")\n",
    "        \n",
    "        # Create scatter plot\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(future_predictions[price_col_future], future_predictions['Demand'], alpha=0.5)\n",
    "        plt.title('Future Price vs Demand Correlation (2025-2029)')\n",
    "        plt.xlabel('Price (EUR/MWh)')\n",
    "        plt.ylabel('Demand (MWh)')\n",
    "        plt.grid(True)\n",
    "        plt.savefig(os.path.join(IMAGES_DIR, 'future_price_demand_correlation.png'), dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "        # Monthly average demand by year\n",
    "        future_predictions['Year'] = pd.to_datetime(future_predictions['Date']).dt.year\n",
    "        future_predictions['Month'] = pd.to_datetime(future_predictions['Date']).dt.month\n",
    "        \n",
    "        monthly_avg = future_predictions.groupby(['Year', 'Month'])['Demand'].mean().reset_index()\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for year in monthly_avg['Year'].unique():\n",
    "            year_data = monthly_avg[monthly_avg['Year'] == year]\n",
    "            plt.plot(year_data['Month'], year_data['Demand'], marker='o', label=str(year))\n",
    "        \n",
    "        plt.title('Monthly Average Demand by Year (2025-2029)')\n",
    "        plt.xlabel('Month')\n",
    "        plt.ylabel('Average Demand (MWh)')\n",
    "        plt.grid(True)\n",
    "        plt.legend()\n",
    "        plt.xticks(range(1, 13))\n",
    "        plt.savefig(os.path.join(IMAGES_DIR, 'monthly_avg_demand_by_year.png'), dpi=300)\n",
    "        plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Could not analyze future correlation: {e}\")\n",
    "\n",
    "def plot_full_demand_history_and_predictions(train_data_hist, test_data_hist, future_predictions_df):\n",
    "    \"\"\"\n",
    "    Plot the full history of energy demand (training and test) and predicted future demand.\n",
    "    \n",
    "    Args:\n",
    "        train_data_hist: DataFrame with historical training data (must include 'date' and 'Demand')\n",
    "        test_data_hist: DataFrame with historical test data (must include 'date' and 'Demand')\n",
    "        future_predictions_df: DataFrame with future demand predictions (must include 'Date' and 'Demand')\n",
    "    \"\"\"\n",
    "    print(\"Generating full demand history and predictions plot...\")\n",
    "\n",
    "    # Prepare training data for plot by explicitly creating a new DataFrame\n",
    "    train_plot = pd.DataFrame({\n",
    "        'Date': pd.to_datetime(train_data_hist['date']),\n",
    "        'Demand': train_data_hist['Demand'],\n",
    "        'Source': 'Training Data'\n",
    "    })\n",
    "\n",
    "    # Prepare test data for plot by explicitly creating a new DataFrame\n",
    "    test_plot = pd.DataFrame({\n",
    "        'Date': pd.to_datetime(test_data_hist['date']),\n",
    "        'Demand': test_data_hist['Demand'],\n",
    "        'Source': 'Test Data'\n",
    "    })\n",
    "\n",
    "    # Prepare future predictions for plot by explicitly creating a new DataFrame\n",
    "    future_plot = pd.DataFrame({\n",
    "        'Date': pd.to_datetime(future_predictions_df['Date']),\n",
    "        'Demand': future_predictions_df['Demand'],\n",
    "        'Source': 'Predicted (2025-2029)'\n",
    "    })\n",
    "\n",
    "    # Concatenate all data for plotting\n",
    "    plot_data = pd.concat([train_plot, test_plot, future_plot], ignore_index=True)\n",
    "    \n",
    "    # Define the order of categories for the 'hue' in the plot legend\n",
    "    hue_order = ['Training Data', 'Test Data', 'Predicted (2025-2029)']\n",
    "    \n",
    "    # Generate the plot\n",
    "    plt.figure(figsize=(18, 9))\n",
    "    sns.lineplot(\n",
    "        x='Date', \n",
    "        y='Demand', \n",
    "        hue='Source', \n",
    "        data=plot_data,\n",
    "        hue_order=hue_order,\n",
    "        linewidth=1.2\n",
    "    )\n",
    "    plt.title('Energy Demand: Historical (Training & Test) and Predicted (2015-2029)')\n",
    "    plt.xlabel('Date')\n",
    "    plt.ylabel('Demand (MWh)')\n",
    "    plt.legend(title='Data Source')\n",
    "    plt.grid(True, alpha=0.4)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    plot_filename = 'demand_full_history_and_predictions.png'\n",
    "    plt.savefig(IMAGES_DIR / plot_filename, dpi=300)\n",
    "    print(f\"Comprehensive demand plot saved to {IMAGES_DIR / plot_filename}\")\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "4a8275dd",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed demand data...\n",
      "Successfully loaded training data: 2557 rows\n",
      "Successfully loaded testing data: 731 rows\n",
      "Preparing features and target variables...\n",
      "Warning: Feature 'is_weekend' not found in training data. Will be removed from feature list.\n",
      "Attempting to use features: ['dayofweek', 'month', 'quarter', 'year', 'dayofyear', 'demand_lag1', 'demand_lag7', 'demand_lag30', 'demand_rolling_7d_mean', 'demand_rolling_30d_mean', 'demand_rolling_7d_std', 'demand_rolling_30d_std', 'price_eur_mwh']\n",
      "Features used: ['dayofweek', 'month', 'quarter', 'year', 'dayofyear', 'demand_lag1', 'demand_lag7', 'demand_lag30', 'demand_rolling_7d_mean', 'demand_rolling_30d_mean', 'demand_rolling_7d_std', 'demand_rolling_30d_std', 'price_eur_mwh']\n",
      "X_train shape: (2557, 13), y_train shape: (2557,)\n",
      "X_test shape: (731, 13), y_test shape: (731,)\n",
      "Training XGBoost model...\n",
      "[0]\tvalidation_0-rmse:4489.58470\n",
      "[1]\tvalidation_0-rmse:4347.33462\n",
      "[2]\tvalidation_0-rmse:4182.24219\n",
      "[3]\tvalidation_0-rmse:4026.11466\n",
      "[4]\tvalidation_0-rmse:3858.27398\n",
      "[5]\tvalidation_0-rmse:3703.27862\n",
      "[6]\tvalidation_0-rmse:3581.78714\n",
      "[7]\tvalidation_0-rmse:3440.64163\n",
      "[8]\tvalidation_0-rmse:3303.66397\n",
      "[9]\tvalidation_0-rmse:3173.67051\n",
      "[10]\tvalidation_0-rmse:3056.60453\n",
      "[11]\tvalidation_0-rmse:2936.35363\n",
      "[12]\tvalidation_0-rmse:2861.64751\n",
      "[13]\tvalidation_0-rmse:2794.47726\n",
      "[14]\tvalidation_0-rmse:2708.34328\n",
      "[15]\tvalidation_0-rmse:2612.51142\n",
      "[16]\tvalidation_0-rmse:2523.10137\n",
      "[17]\tvalidation_0-rmse:2443.65532\n",
      "[18]\tvalidation_0-rmse:2363.81145\n",
      "[19]\tvalidation_0-rmse:2298.87548\n",
      "[20]\tvalidation_0-rmse:2231.62155\n",
      "[21]\tvalidation_0-rmse:2169.07104\n",
      "[22]\tvalidation_0-rmse:2111.34136\n",
      "[23]\tvalidation_0-rmse:2061.31232\n",
      "[24]\tvalidation_0-rmse:2018.66106\n",
      "[25]\tvalidation_0-rmse:1965.43316\n",
      "[26]\tvalidation_0-rmse:1941.84742\n",
      "[27]\tvalidation_0-rmse:1895.63488\n",
      "[28]\tvalidation_0-rmse:1850.13747\n",
      "[29]\tvalidation_0-rmse:1815.49324\n",
      "[30]\tvalidation_0-rmse:1787.58677\n",
      "[31]\tvalidation_0-rmse:1761.97668\n",
      "[32]\tvalidation_0-rmse:1727.43210\n",
      "[33]\tvalidation_0-rmse:1697.31765\n",
      "[34]\tvalidation_0-rmse:1668.52466\n",
      "[35]\tvalidation_0-rmse:1640.45787\n",
      "[36]\tvalidation_0-rmse:1614.42635\n",
      "[37]\tvalidation_0-rmse:1598.54011\n",
      "[38]\tvalidation_0-rmse:1575.36542\n",
      "[39]\tvalidation_0-rmse:1551.20739\n",
      "[40]\tvalidation_0-rmse:1533.99378\n",
      "[41]\tvalidation_0-rmse:1516.11162\n",
      "[42]\tvalidation_0-rmse:1497.02189\n",
      "[43]\tvalidation_0-rmse:1482.45634\n",
      "[44]\tvalidation_0-rmse:1466.23701\n",
      "[45]\tvalidation_0-rmse:1455.48706\n",
      "[46]\tvalidation_0-rmse:1446.03542\n",
      "[47]\tvalidation_0-rmse:1430.90014\n",
      "[48]\tvalidation_0-rmse:1420.14522\n",
      "[49]\tvalidation_0-rmse:1411.56461\n",
      "[50]\tvalidation_0-rmse:1398.53513\n",
      "[51]\tvalidation_0-rmse:1388.05791\n",
      "[52]\tvalidation_0-rmse:1373.53856\n",
      "[53]\tvalidation_0-rmse:1365.75947\n",
      "[54]\tvalidation_0-rmse:1357.06962\n",
      "[55]\tvalidation_0-rmse:1351.50246\n",
      "[56]\tvalidation_0-rmse:1343.64076\n",
      "[57]\tvalidation_0-rmse:1334.80302\n",
      "[58]\tvalidation_0-rmse:1329.11732\n",
      "[59]\tvalidation_0-rmse:1325.52737\n",
      "[60]\tvalidation_0-rmse:1318.50872\n",
      "[61]\tvalidation_0-rmse:1312.59483\n",
      "[62]\tvalidation_0-rmse:1307.43675\n",
      "[63]\tvalidation_0-rmse:1295.43857\n",
      "[64]\tvalidation_0-rmse:1290.38675\n",
      "[65]\tvalidation_0-rmse:1282.60558\n",
      "[66]\tvalidation_0-rmse:1274.44188\n",
      "[67]\tvalidation_0-rmse:1270.99827\n",
      "[68]\tvalidation_0-rmse:1266.10538\n",
      "[69]\tvalidation_0-rmse:1261.48751\n",
      "[70]\tvalidation_0-rmse:1258.37849\n",
      "[71]\tvalidation_0-rmse:1256.19828\n",
      "[72]\tvalidation_0-rmse:1254.24875\n",
      "[73]\tvalidation_0-rmse:1252.87413\n",
      "[74]\tvalidation_0-rmse:1250.35016\n",
      "[75]\tvalidation_0-rmse:1246.45422\n",
      "[76]\tvalidation_0-rmse:1243.68274\n",
      "[77]\tvalidation_0-rmse:1238.89857\n",
      "[78]\tvalidation_0-rmse:1237.44459\n",
      "[79]\tvalidation_0-rmse:1236.66669\n",
      "[80]\tvalidation_0-rmse:1234.55628\n",
      "[81]\tvalidation_0-rmse:1231.50105\n",
      "[82]\tvalidation_0-rmse:1227.98595\n",
      "[83]\tvalidation_0-rmse:1224.62919\n",
      "[84]\tvalidation_0-rmse:1222.01750\n",
      "[85]\tvalidation_0-rmse:1216.85066\n",
      "[86]\tvalidation_0-rmse:1215.01318\n",
      "[87]\tvalidation_0-rmse:1211.57060\n",
      "[88]\tvalidation_0-rmse:1209.64576\n",
      "[89]\tvalidation_0-rmse:1209.26807\n",
      "[90]\tvalidation_0-rmse:1209.30969\n",
      "[91]\tvalidation_0-rmse:1206.39769\n",
      "[92]\tvalidation_0-rmse:1206.13310\n",
      "[93]\tvalidation_0-rmse:1201.66350\n",
      "[94]\tvalidation_0-rmse:1200.16548\n",
      "[95]\tvalidation_0-rmse:1195.40417\n",
      "[96]\tvalidation_0-rmse:1195.93762\n",
      "[97]\tvalidation_0-rmse:1194.42308\n",
      "[98]\tvalidation_0-rmse:1193.86995\n",
      "[99]\tvalidation_0-rmse:1191.12693\n",
      "[100]\tvalidation_0-rmse:1188.46586\n",
      "[101]\tvalidation_0-rmse:1186.16513\n",
      "[102]\tvalidation_0-rmse:1184.40367\n",
      "[103]\tvalidation_0-rmse:1182.56347\n",
      "[104]\tvalidation_0-rmse:1181.64066\n",
      "[105]\tvalidation_0-rmse:1180.23836\n",
      "[106]\tvalidation_0-rmse:1176.46014\n",
      "[107]\tvalidation_0-rmse:1175.67827\n",
      "[108]\tvalidation_0-rmse:1174.35165\n",
      "[109]\tvalidation_0-rmse:1172.35389\n",
      "[110]\tvalidation_0-rmse:1169.56928\n",
      "[111]\tvalidation_0-rmse:1168.47327\n",
      "[112]\tvalidation_0-rmse:1166.51396\n",
      "[113]\tvalidation_0-rmse:1165.90444\n",
      "[114]\tvalidation_0-rmse:1163.16592\n",
      "[115]\tvalidation_0-rmse:1162.39782\n",
      "[116]\tvalidation_0-rmse:1160.39973\n",
      "[117]\tvalidation_0-rmse:1160.34064\n",
      "[118]\tvalidation_0-rmse:1158.63201\n",
      "[119]\tvalidation_0-rmse:1157.55996\n",
      "[120]\tvalidation_0-rmse:1157.76667\n",
      "[121]\tvalidation_0-rmse:1154.81233\n",
      "[122]\tvalidation_0-rmse:1154.16581\n",
      "[123]\tvalidation_0-rmse:1153.56532\n",
      "[124]\tvalidation_0-rmse:1153.43436\n",
      "[125]\tvalidation_0-rmse:1154.27068\n",
      "[126]\tvalidation_0-rmse:1153.81487\n",
      "[127]\tvalidation_0-rmse:1152.88310\n",
      "[128]\tvalidation_0-rmse:1152.82837\n",
      "[129]\tvalidation_0-rmse:1152.87478\n",
      "[130]\tvalidation_0-rmse:1148.58673\n",
      "[131]\tvalidation_0-rmse:1148.24394\n",
      "[132]\tvalidation_0-rmse:1148.01048\n",
      "[133]\tvalidation_0-rmse:1146.37197\n",
      "[134]\tvalidation_0-rmse:1145.29619\n",
      "[135]\tvalidation_0-rmse:1144.56861\n",
      "[136]\tvalidation_0-rmse:1142.02936\n",
      "[137]\tvalidation_0-rmse:1141.56085\n",
      "[138]\tvalidation_0-rmse:1140.51303\n",
      "[139]\tvalidation_0-rmse:1140.27351\n",
      "[140]\tvalidation_0-rmse:1140.76632\n",
      "[141]\tvalidation_0-rmse:1139.59826\n",
      "[142]\tvalidation_0-rmse:1138.90882\n",
      "[143]\tvalidation_0-rmse:1138.36030\n",
      "[144]\tvalidation_0-rmse:1138.37023\n",
      "[145]\tvalidation_0-rmse:1138.08761\n",
      "[146]\tvalidation_0-rmse:1137.86288\n",
      "[147]\tvalidation_0-rmse:1135.26460\n",
      "[148]\tvalidation_0-rmse:1133.49912\n",
      "[149]\tvalidation_0-rmse:1133.53663\n",
      "[150]\tvalidation_0-rmse:1133.62659\n",
      "[151]\tvalidation_0-rmse:1131.58394\n",
      "[152]\tvalidation_0-rmse:1130.90295\n",
      "[153]\tvalidation_0-rmse:1128.88289\n",
      "[154]\tvalidation_0-rmse:1127.58433\n",
      "[155]\tvalidation_0-rmse:1127.73361\n",
      "[156]\tvalidation_0-rmse:1127.47206\n",
      "[157]\tvalidation_0-rmse:1125.48813\n",
      "[158]\tvalidation_0-rmse:1124.16265\n",
      "[159]\tvalidation_0-rmse:1123.94442\n",
      "[160]\tvalidation_0-rmse:1123.00388\n",
      "[161]\tvalidation_0-rmse:1123.26889\n",
      "[162]\tvalidation_0-rmse:1122.62493\n",
      "[163]\tvalidation_0-rmse:1122.80461\n",
      "[164]\tvalidation_0-rmse:1121.30531\n",
      "[165]\tvalidation_0-rmse:1120.78146\n",
      "[166]\tvalidation_0-rmse:1120.71212\n",
      "[167]\tvalidation_0-rmse:1121.07921\n",
      "[168]\tvalidation_0-rmse:1120.18590\n",
      "[169]\tvalidation_0-rmse:1119.26470\n",
      "[170]\tvalidation_0-rmse:1119.98883\n",
      "[171]\tvalidation_0-rmse:1119.72781\n",
      "[172]\tvalidation_0-rmse:1120.25541\n",
      "[173]\tvalidation_0-rmse:1119.73557\n",
      "[174]\tvalidation_0-rmse:1118.25309\n",
      "[175]\tvalidation_0-rmse:1117.94739\n",
      "[176]\tvalidation_0-rmse:1117.57493\n",
      "[177]\tvalidation_0-rmse:1117.09757\n",
      "[178]\tvalidation_0-rmse:1114.68981\n",
      "[179]\tvalidation_0-rmse:1113.88619\n",
      "[180]\tvalidation_0-rmse:1113.05117\n",
      "[181]\tvalidation_0-rmse:1112.61233\n",
      "[182]\tvalidation_0-rmse:1112.42932\n",
      "[183]\tvalidation_0-rmse:1112.31227\n",
      "[184]\tvalidation_0-rmse:1112.30050\n",
      "[185]\tvalidation_0-rmse:1112.32857\n",
      "[186]\tvalidation_0-rmse:1111.64713\n",
      "[187]\tvalidation_0-rmse:1111.37193\n",
      "[188]\tvalidation_0-rmse:1111.41510\n",
      "[189]\tvalidation_0-rmse:1110.97978\n",
      "[190]\tvalidation_0-rmse:1110.83262\n",
      "[191]\tvalidation_0-rmse:1110.09618\n",
      "[192]\tvalidation_0-rmse:1110.14617\n",
      "[193]\tvalidation_0-rmse:1109.96969\n",
      "[194]\tvalidation_0-rmse:1108.96957\n",
      "[195]\tvalidation_0-rmse:1109.14583\n",
      "[196]\tvalidation_0-rmse:1108.72719\n",
      "[197]\tvalidation_0-rmse:1108.79733\n",
      "[198]\tvalidation_0-rmse:1108.39686\n",
      "[199]\tvalidation_0-rmse:1107.81334\n",
      "[200]\tvalidation_0-rmse:1107.72643\n",
      "[201]\tvalidation_0-rmse:1106.65854\n",
      "[202]\tvalidation_0-rmse:1107.29366\n",
      "[203]\tvalidation_0-rmse:1107.26455\n",
      "[204]\tvalidation_0-rmse:1107.17499\n",
      "[205]\tvalidation_0-rmse:1106.24432\n",
      "[206]\tvalidation_0-rmse:1105.51098\n",
      "[207]\tvalidation_0-rmse:1105.96327\n",
      "[208]\tvalidation_0-rmse:1106.08321\n",
      "[209]\tvalidation_0-rmse:1105.56763\n",
      "[210]\tvalidation_0-rmse:1105.43941\n",
      "[211]\tvalidation_0-rmse:1105.62341\n",
      "[212]\tvalidation_0-rmse:1105.53597\n",
      "[213]\tvalidation_0-rmse:1104.64644\n",
      "[214]\tvalidation_0-rmse:1104.75126\n",
      "[215]\tvalidation_0-rmse:1104.61222\n",
      "[216]\tvalidation_0-rmse:1104.20273\n",
      "[217]\tvalidation_0-rmse:1103.95617\n",
      "[218]\tvalidation_0-rmse:1103.05151\n",
      "[219]\tvalidation_0-rmse:1103.38584\n",
      "[220]\tvalidation_0-rmse:1104.14000\n",
      "[221]\tvalidation_0-rmse:1104.61700\n",
      "[222]\tvalidation_0-rmse:1104.82026\n",
      "[223]\tvalidation_0-rmse:1104.19960\n",
      "[224]\tvalidation_0-rmse:1103.91777\n",
      "[225]\tvalidation_0-rmse:1102.87283\n",
      "[226]\tvalidation_0-rmse:1103.27980\n",
      "[227]\tvalidation_0-rmse:1102.23992\n",
      "[228]\tvalidation_0-rmse:1102.02547\n",
      "[229]\tvalidation_0-rmse:1101.89391\n",
      "[230]\tvalidation_0-rmse:1100.86617\n",
      "[231]\tvalidation_0-rmse:1101.14442\n",
      "[232]\tvalidation_0-rmse:1100.94348\n",
      "[233]\tvalidation_0-rmse:1100.38606\n",
      "[234]\tvalidation_0-rmse:1099.28178\n",
      "[235]\tvalidation_0-rmse:1099.06651\n",
      "[236]\tvalidation_0-rmse:1099.33856\n",
      "[237]\tvalidation_0-rmse:1098.57278\n",
      "[238]\tvalidation_0-rmse:1098.72127\n",
      "[239]\tvalidation_0-rmse:1098.56014\n",
      "[240]\tvalidation_0-rmse:1098.82404\n",
      "[241]\tvalidation_0-rmse:1097.76527\n",
      "[242]\tvalidation_0-rmse:1096.89592\n",
      "[243]\tvalidation_0-rmse:1096.79722\n",
      "[244]\tvalidation_0-rmse:1096.34490\n",
      "[245]\tvalidation_0-rmse:1095.63210\n",
      "[246]\tvalidation_0-rmse:1095.58401\n",
      "[247]\tvalidation_0-rmse:1096.07639\n",
      "[248]\tvalidation_0-rmse:1095.11931\n",
      "[249]\tvalidation_0-rmse:1095.02137\n",
      "[250]\tvalidation_0-rmse:1095.16424\n",
      "[251]\tvalidation_0-rmse:1095.48412\n",
      "[252]\tvalidation_0-rmse:1094.94512\n",
      "[253]\tvalidation_0-rmse:1094.45773\n",
      "[254]\tvalidation_0-rmse:1094.02573\n",
      "[255]\tvalidation_0-rmse:1093.91157\n",
      "[256]\tvalidation_0-rmse:1093.17721\n",
      "[257]\tvalidation_0-rmse:1092.79986\n",
      "[258]\tvalidation_0-rmse:1091.66047\n",
      "[259]\tvalidation_0-rmse:1091.46333\n",
      "[260]\tvalidation_0-rmse:1091.15051\n",
      "[261]\tvalidation_0-rmse:1090.63351\n",
      "[262]\tvalidation_0-rmse:1090.60452\n",
      "[263]\tvalidation_0-rmse:1090.52427\n",
      "[264]\tvalidation_0-rmse:1090.78234\n",
      "[265]\tvalidation_0-rmse:1090.86693\n",
      "[266]\tvalidation_0-rmse:1091.08605\n",
      "[267]\tvalidation_0-rmse:1091.44386\n",
      "[268]\tvalidation_0-rmse:1091.34568\n",
      "[269]\tvalidation_0-rmse:1091.02711\n",
      "[270]\tvalidation_0-rmse:1091.32302\n",
      "[271]\tvalidation_0-rmse:1091.35983\n",
      "[272]\tvalidation_0-rmse:1091.91232\n",
      "[273]\tvalidation_0-rmse:1091.91182\n",
      "[274]\tvalidation_0-rmse:1091.65488\n",
      "[275]\tvalidation_0-rmse:1091.17744\n",
      "[276]\tvalidation_0-rmse:1091.19346\n",
      "[277]\tvalidation_0-rmse:1091.30154\n",
      "[278]\tvalidation_0-rmse:1091.60418\n",
      "[279]\tvalidation_0-rmse:1091.91838\n",
      "[280]\tvalidation_0-rmse:1091.70009\n",
      "[281]\tvalidation_0-rmse:1091.51849\n",
      "[282]\tvalidation_0-rmse:1091.84656\n",
      "[283]\tvalidation_0-rmse:1091.53042\n",
      "[284]\tvalidation_0-rmse:1091.73943\n",
      "[285]\tvalidation_0-rmse:1091.58361\n",
      "[286]\tvalidation_0-rmse:1090.38341\n",
      "[287]\tvalidation_0-rmse:1090.56058\n",
      "[288]\tvalidation_0-rmse:1090.38482\n",
      "[289]\tvalidation_0-rmse:1090.33012\n",
      "[290]\tvalidation_0-rmse:1090.67233\n",
      "[291]\tvalidation_0-rmse:1090.70348\n",
      "[292]\tvalidation_0-rmse:1090.16855\n",
      "[293]\tvalidation_0-rmse:1090.20729\n",
      "[294]\tvalidation_0-rmse:1089.89864\n",
      "[295]\tvalidation_0-rmse:1088.29613\n",
      "[296]\tvalidation_0-rmse:1088.09751\n",
      "[297]\tvalidation_0-rmse:1088.26763\n",
      "[298]\tvalidation_0-rmse:1088.51216\n",
      "[299]\tvalidation_0-rmse:1088.65174\n",
      "[300]\tvalidation_0-rmse:1088.62139\n",
      "[301]\tvalidation_0-rmse:1088.58313\n",
      "[302]\tvalidation_0-rmse:1088.73043\n",
      "[303]\tvalidation_0-rmse:1088.63323\n",
      "[304]\tvalidation_0-rmse:1088.18102\n",
      "[305]\tvalidation_0-rmse:1088.21820\n",
      "[306]\tvalidation_0-rmse:1088.45932\n",
      "[307]\tvalidation_0-rmse:1088.19254\n",
      "[308]\tvalidation_0-rmse:1088.00529\n",
      "[309]\tvalidation_0-rmse:1087.77429\n",
      "[310]\tvalidation_0-rmse:1088.00041\n",
      "[311]\tvalidation_0-rmse:1088.05657\n",
      "[312]\tvalidation_0-rmse:1087.85219\n",
      "[313]\tvalidation_0-rmse:1088.38598\n",
      "[314]\tvalidation_0-rmse:1088.13015\n",
      "[315]\tvalidation_0-rmse:1088.15213\n",
      "[316]\tvalidation_0-rmse:1087.92885\n",
      "[317]\tvalidation_0-rmse:1088.11757\n",
      "[318]\tvalidation_0-rmse:1088.11376\n",
      "[319]\tvalidation_0-rmse:1088.12467\n",
      "[320]\tvalidation_0-rmse:1088.08608\n",
      "[321]\tvalidation_0-rmse:1087.70135\n",
      "[322]\tvalidation_0-rmse:1086.55081\n",
      "[323]\tvalidation_0-rmse:1086.29483\n",
      "[324]\tvalidation_0-rmse:1086.33595\n",
      "[325]\tvalidation_0-rmse:1086.16511\n",
      "[326]\tvalidation_0-rmse:1086.39417\n",
      "[327]\tvalidation_0-rmse:1086.63366\n",
      "[328]\tvalidation_0-rmse:1086.33928\n",
      "[329]\tvalidation_0-rmse:1085.89379\n",
      "[330]\tvalidation_0-rmse:1085.92409\n",
      "[331]\tvalidation_0-rmse:1085.81017\n",
      "[332]\tvalidation_0-rmse:1085.73630\n",
      "[333]\tvalidation_0-rmse:1085.21748\n",
      "[334]\tvalidation_0-rmse:1085.44869\n",
      "[335]\tvalidation_0-rmse:1085.52493\n",
      "[336]\tvalidation_0-rmse:1085.66075\n",
      "[337]\tvalidation_0-rmse:1086.38611\n",
      "[338]\tvalidation_0-rmse:1086.31175\n",
      "[339]\tvalidation_0-rmse:1085.90724\n",
      "[340]\tvalidation_0-rmse:1085.97017\n",
      "[341]\tvalidation_0-rmse:1086.00848\n",
      "[342]\tvalidation_0-rmse:1085.84517\n",
      "[343]\tvalidation_0-rmse:1086.21334\n",
      "[344]\tvalidation_0-rmse:1086.22419\n",
      "[345]\tvalidation_0-rmse:1086.51930\n",
      "[346]\tvalidation_0-rmse:1085.92044\n",
      "[347]\tvalidation_0-rmse:1085.55374\n",
      "[348]\tvalidation_0-rmse:1085.81803\n",
      "[349]\tvalidation_0-rmse:1085.84435\n",
      "[350]\tvalidation_0-rmse:1085.72359\n",
      "[351]\tvalidation_0-rmse:1085.86051\n",
      "[352]\tvalidation_0-rmse:1086.17603\n",
      "[353]\tvalidation_0-rmse:1086.30921\n",
      "[354]\tvalidation_0-rmse:1086.37747\n",
      "[355]\tvalidation_0-rmse:1086.38049\n",
      "[356]\tvalidation_0-rmse:1086.38068\n",
      "[357]\tvalidation_0-rmse:1086.54266\n",
      "[358]\tvalidation_0-rmse:1086.52173\n",
      "[359]\tvalidation_0-rmse:1086.47883\n",
      "[360]\tvalidation_0-rmse:1086.25958\n",
      "[361]\tvalidation_0-rmse:1086.08734\n",
      "[362]\tvalidation_0-rmse:1086.27756\n",
      "[363]\tvalidation_0-rmse:1085.81527\n",
      "[364]\tvalidation_0-rmse:1085.87078\n",
      "[365]\tvalidation_0-rmse:1085.92980\n",
      "[366]\tvalidation_0-rmse:1085.87240\n",
      "[367]\tvalidation_0-rmse:1085.76048\n",
      "[368]\tvalidation_0-rmse:1085.94359\n",
      "[369]\tvalidation_0-rmse:1085.99381\n",
      "[370]\tvalidation_0-rmse:1085.78046\n",
      "[371]\tvalidation_0-rmse:1085.91644\n",
      "[372]\tvalidation_0-rmse:1085.85195\n",
      "[373]\tvalidation_0-rmse:1086.38231\n",
      "[374]\tvalidation_0-rmse:1086.38705\n",
      "[375]\tvalidation_0-rmse:1086.59498\n",
      "[376]\tvalidation_0-rmse:1086.53488\n",
      "[377]\tvalidation_0-rmse:1086.69452\n",
      "[378]\tvalidation_0-rmse:1086.66373\n",
      "[379]\tvalidation_0-rmse:1086.79041\n",
      "[380]\tvalidation_0-rmse:1087.04291\n",
      "[381]\tvalidation_0-rmse:1086.87507\n",
      "[382]\tvalidation_0-rmse:1086.99491\n",
      "[383]\tvalidation_0-rmse:1087.38612\n",
      "Test R² score: 0.9430\n",
      "Model saved to /Users/aless/Desktop/AI_ML_Projects/DataCenters/project/models/energy_demand_xgb_v1.joblib\n",
      "Evaluating model performance...\n",
      "RMSE: 1085.22 MWh\n",
      "MAE: 709.27 MWh\n",
      "R²: 0.9430\n",
      "Loading price forecasts for 2025-2029...\n",
      "Successfully loaded price forecasts: 1826 rows\n",
      "Generating future demand predictions for 2025-2029...\n",
      "Progress: 100/1826 days processed\n",
      "Progress: 200/1826 days processed\n",
      "Progress: 300/1826 days processed\n",
      "Progress: 400/1826 days processed\n",
      "Progress: 500/1826 days processed\n",
      "Progress: 600/1826 days processed\n",
      "Progress: 700/1826 days processed\n",
      "Progress: 800/1826 days processed\n",
      "Progress: 900/1826 days processed\n",
      "Progress: 1000/1826 days processed\n",
      "Progress: 1100/1826 days processed\n",
      "Progress: 1200/1826 days processed\n",
      "Progress: 1300/1826 days processed\n",
      "Progress: 1400/1826 days processed\n",
      "Progress: 1500/1826 days processed\n",
      "Progress: 1600/1826 days processed\n",
      "Progress: 1700/1826 days processed\n",
      "Progress: 1800/1826 days processed\n",
      "Progress: 1826/1826 days processed\n",
      "Used 'price_eur_mwh' as the price column\n",
      "Future demand predictions saved to /Users/aless/Desktop/AI_ML_Projects/DataCenters/project/data/final/Italy/energy_demand2025_2029.csv\n",
      "Analyzing correlations between price and demand...\n",
      "Historical correlation between price and demand: 0.1250\n",
      "Future correlation between price and demand: 0.4067\n",
      "Generating full demand history and predictions plot...\n",
      "Comprehensive demand plot saved to /Users/aless/Desktop/AI_ML_Projects/DataCenters/project/outputs/images/demand_full_history_and_predictions.png\n",
      "Energy Demand Forecasting Model Training completed successfully!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data, test_data = load_data()\n",
    "\n",
    "# Prepare features and target\n",
    "X_train, y_train, X_test, y_test, features = prepare_features_target(train_data, test_data)\n",
    "\n",
    "# Train model\n",
    "model = train_model(X_train, y_train, X_test, y_test)\n",
    "\n",
    "# Evaluate model\n",
    "evaluate_model(model, X_test, y_test, features)\n",
    "\n",
    "# Load price forecasts\n",
    "price_forecasts = load_price_forecasts()\n",
    "\n",
    "# Generate future predictions\n",
    "future_predictions = generate_future_predictions(model, features, price_forecasts)\n",
    "\n",
    "# Analyze correlations\n",
    "if future_predictions is not None:\n",
    "    analyze_correlations(train_data, future_predictions)\n",
    "    plot_full_demand_history_and_predictions(train_data, test_data, future_predictions)\n",
    "\n",
    "print(\"Energy Demand Forecasting Model Training completed successfully!\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "encoding": "# -*- coding: utf-8 -*-",
   "executable": "/usr/bin/env python3",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
